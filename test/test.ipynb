{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8befcac1-e121-43f9-947a-226eb17e1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation.logits_process import LogitsProcessorList, InfNanRemoveLogitsProcessor\n",
    "from transformers_gad.grammar_utils import IncrementalGrammarConstraint\n",
    "from transformers_gad.generation.logits_process import GrammarAlignedOracleLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36864bf-3aa6-436a-ac1e-732395502ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiry/.conda/envs/GAD/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "NUM_ITER = 10\n",
    "MODEL_ID = \"TinyLlama/TinyLlama_v1.1\"\n",
    "TRIE_PATH = \"tries/binary_len_5_0_trie.json\"\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.bfloat16\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 1.0\n",
    "REPETITION_PENALTY = 1.0\n",
    "TOP_P = 1.0\n",
    "TOP_K = 0\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "model.to(device)\n",
    "model.to(dtype=DTYPE)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "GRAMMAR_PATH = \"../examples/grammars/arithmetic.ebnf\"\n",
    "# Load EBNF grammar\n",
    "with open(GRAMMAR_PATH, \"r\") as file:\n",
    "    grammar_str = file.read()\n",
    "grammar = IncrementalGrammarConstraint(grammar_str, \"root\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24475b49-ed62-469d-a172-1e9bcd8ada06",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_PATH = \"../examples/test/binary_len_5_0.ebnf\"\n",
    "# Load EBNF grammar\n",
    "with open(GRAMMAR_PATH, \"r\") as file:\n",
    "    grammar_str = file.read()\n",
    "grammar = IncrementalGrammarConstraint(grammar_str, \"root\", tokenizer)\n",
    "\n",
    "# Initialize logits processor for the grammar\n",
    "gad_oracle_processor = GrammarAlignedOracleLogitsProcessor(grammar)\n",
    "inf_nan_remove_processor = InfNanRemoveLogitsProcessor()\n",
    "logits_processors = LogitsProcessorList([\n",
    "    inf_nan_remove_processor,\n",
    "    gad_oracle_processor,\n",
    "])\n",
    "\n",
    "# Tokenize prompt into ids\n",
    "prompt = \"Generate a binary string of length 5.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08627a9-6137-43ba-a582-fb6ea8786264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_prob(s: str) -> float:\n",
    "    full_text = prompt + \" \" + s\n",
    "    tokens = tokenizer.encode(full_text, return_tensors='pt')\n",
    "    start_index = len(tokenizer.encode(prompt + \" \", return_tensors=\"pt\")[0]) - 1\n",
    "    print(start_index)\n",
    "    llm_prob = 1.0\n",
    "    for i in range(start_index, len(tokens[0]) - 1):\n",
    "        input_tokens = tokens[:, :i+1].to(DEVICE)\n",
    "        outputs = model(input_tokens)\n",
    "        logits = outputs.logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        next_token_id = tokens[0, i + 1]\n",
    "        next_token_prob = probabilities[next_token_id].item()\n",
    "        context = tokenizer.decode(input_tokens[0])\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "        llm_prob *= next_token_prob\n",
    "    \n",
    "        #print(f\"Context: '{context}', Actual next token: '{next_token}', Probability: {next_token_prob:.4f}\")\n",
    "    return llm_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0d40b8-8449-4530-bf58-138c4988313b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39meos_token_id]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Generate continuation using beam search with prefix enforcement\u001b[39;00m\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m---> 60\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[43minput_ids\u001b[49m,\n\u001b[1;32m     61\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS,  \u001b[38;5;66;03m# Generate 2 tokens after the prefix to reach length 5\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mTEMPERATURE,\n\u001b[1;32m     63\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mREPETITION_PENALTY,\n\u001b[1;32m     64\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mTOP_P,\n\u001b[1;32m     65\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mTOP_K,\n\u001b[1;32m     66\u001b[0m     output_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,                    \u001b[38;5;66;03m# Beam search\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m#logits_processor=logits_processors,     # Apply grammar and other constraints\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                         \u001b[38;5;66;03m# Deterministic beam search\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,    \u001b[38;5;66;03m# Define padding token\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn  \u001b[38;5;66;03m# Enforce prefix constraints\u001b[39;00m\n\u001b[1;32m     73\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "DESIRED_PREFIX = \"110\"\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(DEVICE)\n",
    "# Encode the desired prefix\n",
    "desired_prefix_ids = tokenizer.encode(DESIRED_PREFIX, add_special_tokens=False)\n",
    "\n",
    "with open(GRAMMAR_PATH, \"r\") as file:\n",
    "    grammar_str = file.read()\n",
    "grammar = IncrementalGrammarConstraint(grammar_str, \"root\", tokenizer)\n",
    "gad_oracle_processor = GrammarAlignedOracleLogitsProcessor(grammar)\n",
    "inf_nan_remove_processor = InfNanRemoveLogitsProcessor()\n",
    "logits_processors = LogitsProcessorList([\n",
    "    inf_nan_remove_processor,\n",
    "    #gcd_oracle_processor,\n",
    "])\n",
    "\n",
    "\n",
    "# Define the prefix_allowed_tokens_fn with correct signature\n",
    "def prefix_allowed_tokens_fn(batch_id: int, input_ids_partial: torch.LongTensor):\n",
    "    #print(input_ids_partial)\n",
    "    #print(tokenizer.decode(input_ids_partial))\n",
    "    #print(input_ids_partial.shape)\n",
    "    #print(tokenizer.encode('0', add_special_tokens=False)[0])\n",
    "    \"\"\"\n",
    "    Restricts the generated tokens to enforce a specific prefix.\n",
    "    \n",
    "    Args:\n",
    "        batch_id (int): The index of the current batch.\n",
    "        input_ids_partial (torch.LongTensor): The input IDs generated so far, including the prompt.\n",
    "        step (int): The current generation step.\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: A list of allowed token IDs for the current step.\n",
    "    \"\"\"\n",
    "    step = input_ids_partial.shape[0]\n",
    "    # Total tokens in the prompt\n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    # Calculate the step relative to the prefix\n",
    "    generation_step = step - prompt_length\n",
    "\n",
    "    if generation_step < 0:\n",
    "        # Still generating the prompt; no restrictions\n",
    "        return list(range(tokenizer.vocab_size))\n",
    "    elif generation_step < len(desired_prefix_ids):\n",
    "        # Enforce the desired prefix\n",
    "        #print(desired_prefix_ids[generation_step])\n",
    "        return [desired_prefix_ids[generation_step]]\n",
    "    elif generation_step < len(desired_prefix_ids) + 5 - len(DESIRED_PREFIX):\n",
    "        # After prefix, allow only '0' or '1' to complete the binary string\n",
    "        token_0 = tokenizer.encode('0', add_special_tokens=False)[1]\n",
    "        token_1 = tokenizer.encode('1', add_special_tokens=False)[1]\n",
    "        return [token_0, token_1]\n",
    "    else:\n",
    "        return [tokenizer.eos_token_id]\n",
    "\n",
    "# Generate continuation using beam search with prefix enforcement\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,  # Generate 2 tokens after the prefix to reach length 5\n",
    "    temperature=TEMPERATURE,\n",
    "    repetition_penalty=REPETITION_PENALTY,\n",
    "    top_p=TOP_P,\n",
    "    top_k=TOP_K,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    num_beams=1,                    # Beam search\n",
    "    #logits_processor=logits_processors,     # Apply grammar and other constraints\n",
    "    do_sample=False,                         # Deterministic beam search\n",
    "    pad_token_id=tokenizer.eos_token_id,    # Define padding token\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn  # Enforce prefix constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b16ff32-d5f4-4ed7-8ea6-57bbef7f9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_gad.recognizer import AcceptState\n",
    "import random\n",
    "RAW, MASKED = \"raw\", \"masked\"\n",
    "EPS = 10 ** -9\n",
    "class SampleNode:\n",
    "    def __init__(self, prefix, state: AcceptState, raw_prob, masked_prob, parent=None, last_token=None):\n",
    "        self.prefix = prefix\n",
    "        self.state = state\n",
    "        self.prefix_prob = {RAW: float(raw_prob), MASKED: float(masked_prob)}\n",
    "        self.sum_prob = None\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.step_prob = {}\n",
    "        self.last_token = last_token\n",
    "\n",
    "    def is_new(self):\n",
    "        return self.sum_prob is None\n",
    "\n",
    "    def to_string(self, tokenizer):\n",
    "        return f\"{tokenizer.decode(self.prefix[0])}@{self.prefix_prob}\"\n",
    "\n",
    "    def insert_child(self, token, new_state, step_prob):\n",
    "        token_tensor = torch.tensor([[token]], device=self.prefix.device)\n",
    "        new_prefix = torch.cat((self.prefix, token_tensor), dim=1)\n",
    "\n",
    "        child = SampleNode(new_prefix, new_state, self.prefix_prob[RAW] * step_prob, \n",
    "                           self.prefix_prob[MASKED] * step_prob / self.sum_prob, self, token)\n",
    "        self.children[token] = child\n",
    "        self.step_prob[token] = step_prob\n",
    "\n",
    "    def sample_next(self, forbid=None):\n",
    "        items = [token for token in self.children if token != forbid]\n",
    "        weights = [self.step_prob[token] for token in items]\n",
    "        #print(items, weights, \"forbid\", forbid)\n",
    "        token = random.choices(items, weights=weights, k=1)[0]\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08de849-ba1e-4f49-a966-9fdb26ee6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10011</s>@{'raw': 2.0651822474349046e-09, 'masked': 0.022224845364689827}\n",
      "<s> Generate a binary string of length 5. 10101</s>@{'raw': 2.4343240756508067e-09, 'masked': 0.025169778615236282}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10001</s>@{'raw': 6.358450654886383e-09, 'masked': 0.015170187689363956}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10001</s>@{'raw': 6.358450654886383e-09, 'masked': 0.015170187689363956}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 10101</s>@{'raw': 2.4343240756508067e-09, 'masked': 0.025169778615236282}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 11111</s>@{'raw': 1.4683114279989695e-08, 'masked': 0.0819893479347229}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 10000</s>@{'raw': 6.539553254469865e-08, 'masked': 0.313358336687088}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n",
      "<s> Generate a binary string of length 5. 00000</s>@{'raw': 7.032596016642856e-08, 'masked': 0.3923368453979492}\n"
     ]
    }
   ],
   "source": [
    "def _process_score(scores):\n",
    "    scores[scores != scores] = 0.0\n",
    "\n",
    "    scores[scores == float(\"inf\")] = torch.finfo(scores.dtype).max\n",
    "    scores[scores == float(\"-inf\")] = torch.finfo(scores.dtype).min\n",
    "    return F.softmax(scores, dim=-1)\n",
    "\n",
    "MAX_TURN = 512\n",
    "\n",
    "def _get_meaningful_parents(node):\n",
    "    parent_list = []\n",
    "    while node.parent is not None:\n",
    "        last_token = node.last_token\n",
    "        node = node.parent\n",
    "        if len(node.children) > 1:\n",
    "            parent_list.append((node, last_token))\n",
    "    return parent_list\n",
    "\n",
    "class SampleHolder:\n",
    "    def __init__(self, model, tokenizer, constraint, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.constraint = constraint\n",
    "        self.device = device\n",
    "        root_tokens = tokenizer.encode(prompt + \" \", return_tensors='pt').to(self.device)\n",
    "        state = grammar.string_recognizer.get_initial_accept_state()\n",
    "        self.root = SampleNode(root_tokens, state, 1.0, 1.0, None)\n",
    "\n",
    "    def draw(self, node: SampleNode = None):\n",
    "        if node is None: node = self.root\n",
    "        for _ in range(MAX_TURN):\n",
    "            #print(\"Sampling from \", node.to_string(self.tokenizer))\n",
    "            if node.is_new():\n",
    "                logits = self.model(node.prefix).logits\n",
    "                raw_score = logits[0, -1, :]\n",
    "                vocab = self.constraint.filter_vocab(node.state, self.device)\n",
    "                \n",
    "                step_tokens = vocab.nonzero().cpu().tolist() # [[x] for a possible token x]            \n",
    "                raw_prob = _process_score(raw_score)\n",
    "    \n",
    "                sum_prob = sum([raw_prob[index[0]] for index in step_tokens])\n",
    "                assert sum_prob > EPS\n",
    "                node.sum_prob = sum_prob\n",
    "                \n",
    "                for _token in step_tokens:\n",
    "                    token = _token[0]\n",
    "                    prob = raw_prob[token].cpu()\n",
    "                    if prob / sum_prob < EPS: continue\n",
    "                    node.insert_child(token, self.constraint._consume_token_id(token, node.state), raw_prob[token].cpu())\n",
    "            token = node.sample_next()\n",
    "            node = node.children[token]\n",
    "            if token == self.tokenizer.eos_token_id:\n",
    "                return node\n",
    "        assert False\n",
    "\n",
    "    def mutate(self, node):\n",
    "        parents = _get_meaningful_parents(node)\n",
    "        if len(parents) == 0: return node\n",
    "        parent, token_x = random.choice(parents)\n",
    "        #print(\"Parent \", parent.to_string(self.tokenizer))\n",
    "        #print(token_x, parent.step_prob)\n",
    "        token_y = parent.sample_next(token_x)\n",
    "        node_y = self.draw(parent.children[token_y])\n",
    "        parents_y = _get_meaningful_parents(node_y)\n",
    "\n",
    "        phi_x, phi_y = node.prefix_prob[RAW], node_y.prefix_prob[RAW]\n",
    "        #print(float(node.prefix_prob[MASKED]), float(node_y.prefix_prob[MASKED]), float(parent.prefix_prob[MASKED]))\n",
    "        #print(float(parent.sum_prob), float(parent.step_prob[token_y]))\n",
    "        trans_xy = 1 / len(parents) * node_y.prefix_prob[MASKED] / (parent.sum_prob - parent.step_prob[token_x]) #/ parent.prefix_prob[MASKED]\n",
    "        trans_yx = 1 / len(parents_y) * node.prefix_prob[MASKED] / (parent.sum_prob - parent.step_prob[token_y]) #/ parent.prefix_prob[MASKED]\n",
    "\n",
    "        accept_prob = phi_y * trans_yx / phi_x / trans_xy\n",
    "        #print(\"transition\")\n",
    "        #print(\"  \", node.to_string(self.tokenizer))\n",
    "        #print(\"  \", node_y.to_string(self.tokenizer))\n",
    "        #print(\"  \", float(accept_prob), float(trans_xy), float(trans_yx))\n",
    "        if random.random() < accept_prob:\n",
    "            return node_y\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    def mcmc(self, round):\n",
    "        x = self.draw()\n",
    "        for _ in range(round):\n",
    "            x = self.mutate(x)\n",
    "        return x\n",
    "        \n",
    "holder = SampleHolder(model, tokenizer, grammar, DEVICE)\n",
    "for i in range(100):\n",
    "    res = holder.mcmc(100)\n",
    "    print(res.to_string(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dda77a-be71-4def-a573-1a403fe5e5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e05ad-23d0-443f-8cd0-be34a73984e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7421679-074c-4bfb-aab8-1bb8a111b1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c329a245-6f75-4e13-8239-48e9c6c64b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本: Generate a binary string of length 5 11011\n"
     ]
    }
   ],
   "source": [
    "generated_ids = output.sequences[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"生成的文本: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d00964-d4c6-4484-9bd4-b200b9d3bc42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
